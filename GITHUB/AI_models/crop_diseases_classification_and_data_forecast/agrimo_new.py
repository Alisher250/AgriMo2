# -*- coding: utf-8 -*-
"""AgriMo_new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12z5-IWx3KlLHnrwaeJSE_1WLjp7nhtzD

### SEQUENTIAL MODEL - CROP DATA PREDICTING(WEATHER, SOIL)
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os
COLAB_PATH = "gdrive/MyDrive"
os.chdir(COLAB_PATH)

# Commented out IPython magic to ensure Python compatibility.
# %cd /content
!ls

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

df = pd.read_csv('combined_data.csv')

X = df[["Radiation","Pressure","WindDirection(Degrees)","Speed","N","P","K","temperature","humidity","ph","rainfall"]]
y = df[["Radiation","Pressure","WindDirection(Degrees)","Speed","N","P","K","temperature","humidity","ph","rainfall"]]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dense(64, activation='relu'),
    layers.Dense(11)
])
model.summary()

model.compile(optimizer='adam', loss='mean_squared_error')
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)

y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean Squared Error:", mse)
print("R-squared (R2) Score:", r2)

model.save("weather_new_ver_1")

keras.utils.plot_model(model, to_file="model_plot.png", show_shapes=True, show_dtype=True)

plt.imshow(plt.imread("model_plot.png"))
plt.axis('off')
plt.show()

plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Mean Squared Error')
plt.legend()

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_pred, alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--', lw=3, color='red')
plt.title('Actual vs. Predicted')
plt.xlabel('Actual Value')
plt.ylabel('Predicted Value')

plt.tight_layout()
plt.show()

def predict_weather(radiation, pressure, wind_direction, wind_speed, nitrogen, phosphorous, potassium, temperature, humidity, ph, rainfall):
    data = np.array([[radiation, pressure, wind_direction, wind_speed, nitrogen, phosphorous, potassium, temperature, humidity, ph, rainfall]])
    data = scaler.transform(data)
    predicted = model.predict(data)
    return predicted

predicted_data = predict_weather(1.21, 30.46, 176.78, 3.37, 85, 58, 41, 20.87974371, 82.00274423, 6.502985292000001, 202.9355362)
print("Predicted Values for New Data:")
print(predicted_data)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

df = pd.read_csv('CombinedData.csv')

print(df.isnull().sum())

import pandas as pd

solar_df = pd.read_csv('SolarPrediction.csv')
crop_df = pd.read_csv('Crop_recommendation.csv')

columns_to_drop_solar = ["UNIXTime", "Data", "Time", "Temperature", "Humidity", "TimeSunRise", "TimeSunSet"]
solar_df = solar_df.drop(columns=columns_to_drop_solar)

crop_df = crop_df.drop(columns=["label"])

combined_df = pd.concat([solar_df, crop_df], axis=1)

combined_df.to_csv('CombinedData.csv', index=False)

combined = pd.read_csv('CombinedData.csv')

combined.head()

import pandas as pd

solar_df = pd.read_csv("SolarPrediction.csv")
crop_df = pd.read_csv("Crop_recommendation.csv")

solar_df.drop(columns=["UNIXTime", "Data", "Time", "Temperature", "Humidity", "TimeSunRise", "TimeSunSet"], inplace=True)
crop_df.drop(columns=["label"], inplace=True)

while len(crop_df) < len(solar_df):
    crop_df = pd.concat([crop_df, crop_df], ignore_index=True)

crop_df = crop_df.iloc[:len(solar_df)]

combined_df = pd.concat([solar_df, crop_df], axis=1)

combined_df.to_csv("combined_data.csv", index=False)

import pandas as pd

combined = pd.read_csv("combined_data.csv")

print(combined.isnull().sum())

"""### CROP DISEASES CLASSIFICATION - RESNET

"""

!pip install torchsummary

# Commented out IPython magic to ensure Python compatibility.
# %cd gdrive/MyDrive/yolov7/
!ls

# Commented out IPython magic to ensure Python compatibility.
import os
import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
import torch.nn as nn
from torch.utils.data import DataLoader
from PIL import Image
import torch.nn.functional as F
import torchvision.transforms as transforms
from torchvision.utils import make_grid
from torchvision.datasets import ImageFolder
from torchsummary import summary

# %matplotlib inline

data_dir = "data/DARYN2023_crop_diseases_kaggle_resnet_new/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)"
train_dir = data_dir + "/train"
valid_dir = data_dir + "/valid"
diseases = os.listdir(valid_dir)

print(diseases)

print("Total disease classes are: {}".format(len(diseases)))

plants = []
NumberOfDiseases = 0
for plant in diseases:
    if plant.split('___')[0] not in plants:
        plants.append(plant.split('___')[0])
    if plant.split('___')[1] != 'healthy':
        NumberOfDiseases += 1

print(f"Unique Plants are: \n{plants}")

print("Number of plants: {}".format(len(plants)))

print("Number of diseases: {}".format(NumberOfDiseases))

nums = {}
for disease in diseases:
    nums[disease] = len(os.listdir(train_dir + '/' + disease))

img_per_class = pd.DataFrame(nums.values(), index=nums.keys(), columns=["no. of images"])
img_per_class

index = [n for n in range(38)]
plt.figure(figsize=(20, 5))
plt.bar(index, [n for n in nums.values()], width=0.3)
plt.xlabel('Plants/Diseases', fontsize=10)
plt.ylabel('No of images available', fontsize=10)
plt.xticks(index, diseases, fontsize=5, rotation=90)
plt.title('Images per each class of plant disease')

n_train = 0
for value in nums.values():
    n_train += value
print(f"There are {n_train} images for training")

!unzip data/crop_diseases_dataset.zip -d ./data/DARYN2023_crop_diseases_kaggle_resnet_new

train = ImageFolder(train_dir, transform=transforms.ToTensor())
valid = ImageFolder(valid_dir, transform=transforms.ToTensor())

print(train_dir)
print(valid_dir)

img, label = train[0]
print(img.shape, label)

len(train.classes)

def show_image(image, label):
    print("Label :" + train.classes[label] + "(" + str(label) + ")")
    plt.imshow(image.permute(1, 2, 0))

show_image(*train[0])

show_image(*train[70000])

random_seed = 7
torch.manual_seed(random_seed)

batch_size = 32

train_dl = DataLoader(train, batch_size, shuffle=True, num_workers=2, pin_memory=True)
valid_dl = DataLoader(valid, batch_size, num_workers=2, pin_memory=True)

def show_batch(data):
    for images, labels in data:
        fig, ax = plt.subplots(figsize=(30, 30))
        ax.set_xticks([]); ax.set_yticks([])
        ax.imshow(make_grid(images, nrow=8).permute(1, 2, 0))
        break

show_batch(train_dl)

def get_default_device():
    """Pick GPU if available, else CPU"""
    if torch.cuda.is_available:
        return torch.device("cuda")
    else:
        return torch.device("cpu")

def to_device(data, device):
    """Move tensor(s) to chosen device"""
    if isinstance(data, (list,tuple)):
        return [to_device(x, device) for x in data]
    return data.to(device, non_blocking=True)

class DeviceDataLoader():
    """Wrap a dataloader to move data to a device"""
    def __init__(self, dl, device):
        self.dl = dl
        self.device = device

    def __iter__(self):
        """Yield a batch of data after moving it to device"""
        for b in self.dl:
            yield to_device(b, self.device)

    def __len__(self):
        """Number of batches"""
        return len(self.dl)

device = get_default_device()
device

train_dl = DeviceDataLoader(train_dl, device)
valid_dl = DeviceDataLoader(valid_dl, device)

!nvidia-smi

import torch
print(torch.cuda.is_available())  # Should print True
print(torch.cuda.device_count())   # Number of GPUs available
print(torch.cuda.get_device_name(0))  # Name of the GPU

class SimpleResidualBlock(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)
        self.relu1 = nn.ReLU()
        self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)
        self.relu2 = nn.ReLU()

    def forward(self, x):
        out = self.conv1(x)
        out = self.relu1(out)
        out = self.conv2(out)
        return self.relu2(out) + x

def accuracy(outputs, labels):
    _, preds = torch.max(outputs, dim=1)
    return torch.tensor(torch.sum(preds == labels).item() / len(preds))


class ImageClassificationBase(nn.Module):

    def training_step(self, batch):
        images, labels = batch
        out = self(images)
        loss = F.cross_entropy(out, labels)
        return loss

    def validation_step(self, batch):
        images, labels = batch
        out = self(images)
        loss = F.cross_entropy(out, labels)
        acc = accuracy(out, labels)
        return {"val_loss": loss.detach(), "val_accuracy": acc}

    def validation_epoch_end(self, outputs):
        batch_losses = [x["val_loss"] for x in outputs]
        batch_accuracy = [x["val_accuracy"] for x in outputs]
        epoch_loss = torch.stack(batch_losses).mean()
        epoch_accuracy = torch.stack(batch_accuracy).mean()
        return {"val_loss": epoch_loss, "val_accuracy": epoch_accuracy}

    def epoch_end(self, epoch, result):
        print("Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}".format(
            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_accuracy']))

def ConvBlock(in_channels, out_channels, pool=False):
    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
             nn.BatchNorm2d(out_channels),
             nn.ReLU(inplace=True)]
    if pool:
        layers.append(nn.MaxPool2d(4))
    return nn.Sequential(*layers)


class ResNet9(ImageClassificationBase):
    def __init__(self, in_channels, num_diseases):
        super().__init__()

        self.conv1 = ConvBlock(in_channels, 64)
        self.conv2 = ConvBlock(64, 128, pool=True)
        self.res1 = nn.Sequential(ConvBlock(128, 128), ConvBlock(128, 128))

        self.conv3 = ConvBlock(128, 256, pool=True)
        self.conv4 = ConvBlock(256, 512, pool=True)
        self.res2 = nn.Sequential(ConvBlock(512, 512), ConvBlock(512, 512))

        self.classifier = nn.Sequential(nn.MaxPool2d(4),
                                       nn.Flatten(),
                                       nn.Linear(512, num_diseases))

    def forward(self, xb):
        out = self.conv1(xb)
        out = self.conv2(out)
        out = self.res1(out) + out
        out = self.conv3(out)
        out = self.conv4(out)
        out = self.res2(out) + out
        out = self.classifier(out)
        return out

model = to_device(ResNet9(3, len(train.classes)), device)
model

INPUT_SHAPE = (3, 256, 256)

import torch
print(torch.cuda.is_available())

@torch.no_grad()
def evaluate(model, val_loader):
    model.eval()
    outputs = [model.validation_step(batch) for batch in val_loader]
    return model.validation_epoch_end(outputs)


def get_lr(optimizer):
    for param_group in optimizer.param_groups:
        return param_group['lr']


def fit_OneCycle(epochs, max_lr, model, train_loader, val_loader, weight_decay=0,
                grad_clip=None, opt_func=torch.optim.SGD):
    torch.cuda.empty_cache()
    history = []

    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)
    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, steps_per_epoch=len(train_loader))


    for epoch in range(epochs):
        model.train()
        train_losses = []
        lrs = []
        for batch in train_loader:
            loss = model.training_step(batch)
            train_losses.append(loss)
            loss.backward()

            if grad_clip:
                nn.utils.clip_grad_value_(model.parameters(), grad_clip)

            optimizer.step()
            optimizer.zero_grad()

            lrs.append(get_lr(optimizer))
            sched.step()


        result = evaluate(model, val_loader)
        result['train_loss'] = torch.stack(train_losses).mean().item()
        result['lrs'] = lrs
        model.epoch_end(epoch, result)
        history.append(result)

    return history

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history = [evaluate(model, valid_dl)]
# history

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

x = torch.randn(1).to(device)

model = model.to(device)

import torch

print(torch.cuda.is_available())
print(torch.cuda.get_device_name(0))

epochs = 2
max_lr = 0.01
grad_clip = 0.1
weight_decay = 1e-4
opt_func = torch.optim.Adam

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history += fit_OneCycle(epochs, max_lr, model, train_dl, valid_dl,
#                              grad_clip=grad_clip,
#                              weight_decay=1e-4,
#                              opt_func=opt_func)

def plot_accuracies(history):
    accuracies = [x['val_accuracy'] for x in history]
    plt.plot(accuracies, '-x')
    plt.xlabel('epoch')
    plt.ylabel('accuracy')
    plt.title('Accuracy vs. No. of epochs');

def plot_losses(history):
    train_losses = [x.get('train_loss') for x in history]
    val_losses = [x['val_loss'] for x in history]
    plt.plot(train_losses, '-bx')
    plt.plot(val_losses, '-rx')
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.legend(['Training', 'Validation'])
    plt.title('Loss vs. No. of epochs');

def plot_lrs(history):
    lrs = np.concatenate([x.get('lrs', []) for x in history])
    plt.plot(lrs)
    plt.xlabel('Batch no.')
    plt.ylabel('Learning rate')
    plt.title('Learning Rate vs. Batch no.');

plot_accuracies(history)

plot_losses(history)

plot_lrs(history)

test_dir = "data/DARYN2023_crop_diseases_kaggle_resnet_new/test"
test = ImageFolder(test_dir, transform=transforms.ToTensor())

!ls

test_images = sorted(os.listdir(test_dir + '/test'))
test_images

def predict_image(img, model):

    xb = to_device(img.unsqueeze(0), device)
    yb = model(xb)
    _, preds  = torch.max(yb, dim=1)

    return train.classes[preds[0].item()]

img, label = test[0]
plt.imshow(img.permute(1, 2, 0))
print('Label:', test_images[0], ', Predicted:', predict_image(img, model))

for i, (img, label) in enumerate(test):
    print('Label:', test_images[i], ', Predicted:', predict_image(img, model))

PATH = 'runs/plant-disease-model.pth'
torch.save(model.state_dict(), PATH)

PATH = 'runs/plant-disease-model-complete.pth'
torch.save(model, PATH)

for i, (img, label) in enumerate(test):
    print('Label:', test_images[i], ', Predicted:', predict_image(img, model))